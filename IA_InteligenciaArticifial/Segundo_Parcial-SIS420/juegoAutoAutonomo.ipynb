{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def create_q_network(input_shape, action_space):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, 8, strides=4, activation='relu'),\n",
    "        layers.Conv2D(64, 4, strides=2, activation='relu'),\n",
    "        layers.Conv2D(64, 3, strides=1, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(action_space, activation='tanh')  # Activación para acciones continuas\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train(episodes):\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode='human')  # Usamos render_mode='human' para mostrar el entorno\n",
    "    print(\"Entorno 'CarRacing-v3' cargado.\")  # Confirmación de que el entorno se ha cargado correctamente\n",
    "\n",
    "    input_shape = env.observation_space.shape\n",
    "    action_space = env.action_space.shape[0]  # Número de acciones continuas\n",
    "\n",
    "    q_network = create_q_network(input_shape, action_space)\n",
    "    target_network = create_q_network(input_shape, action_space)\n",
    "    target_network.set_weights(q_network.get_weights())  # Inicializar con los mismos pesos\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    gamma = 0.99  # Factor de descuento\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    batch_size = 64\n",
    "    memory = deque(maxlen=100000)  # Memoria para experiencia\n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state, dtype=np.float32)  # Convertir el estado a un tipo adecuado\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0  # Contador de pasos para medir el progreso\n",
    "\n",
    "        print(f\"Comenzando episodio {episode + 1}\")  # Confirmación de inicio de episodio\n",
    "\n",
    "        # Establecer el umbral de recompensa para completar el 50% de la pista\n",
    "        max_steps = 1000  # Establecer un número máximo de pasos para todo el episodio\n",
    "        progress_threshold = 0.40  # 50% de progreso\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Exploración\n",
    "            else:\n",
    "                action = q_network.predict(np.expand_dims(state, axis=0)).flatten()  # Explotación\n",
    "\n",
    "            # Ejecutar la acción y obtener el siguiente estado\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            # Acumular la recompensa\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Si el número de pasos supera el 50% de los pasos máximos, finalizar el episodio\n",
    "            if step_count > (progress_threshold * max_steps):\n",
    "                done = True\n",
    "                print(f\"Episodio {episode + 1} terminado porque el agente completó el 50% del recorrido. Recompensa: {episode_reward}\")\n",
    "\n",
    "            # Si la recompensa es muy negativa (el coche salió de la pista), finalizar el episodio\n",
    "            if reward < -50:  # Ajusta este valor si es necesario\n",
    "                done = True\n",
    "                print(f\"Episodio {episode + 1} terminado porque el coche salió de la pista. Recompensa: {reward}\")\n",
    "\n",
    "            # Almacenar la transición en la memoria\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            # Actualizar el estado actual\n",
    "            state = next_state\n",
    "\n",
    "            # Entrenamiento de la red neuronal\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "                states = np.array(states)\n",
    "                next_states = np.array(next_states)\n",
    "\n",
    "                # Predicción de la Q-Value actual\n",
    "                q_values = q_network.predict(states)\n",
    "\n",
    "                # Predicción de la Q-Value objetivo\n",
    "                q_values_next = target_network.predict(next_states)\n",
    "\n",
    "                # Actualizar la Q-Value de la acción tomada\n",
    "                for i in range(batch_size):\n",
    "                    q_values[i] = rewards[i] + gamma * np.max(q_values_next[i]) * (1 - dones[i])\n",
    "\n",
    "                # Entrenamiento de la red\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values_pred = q_network(states)\n",
    "                    loss = tf.reduce_mean(tf.square(q_values_pred - q_values))\n",
    "                grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "        # Reducir epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "        # Mostrar progreso cada 100 episodios\n",
    "        reward_per_episode[episode] = episode_reward\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episodio: {episode + 1} - Recompensa acumulada: {reward_per_episode[episode]}\")\n",
    "\n",
    "        # Actualizar la red objetivo cada 10 episodios\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        # Forzar el reinicio del entorno cuando el episodio se termine\n",
    "        if done:\n",
    "            state, _ = env.reset()  # Reiniciar el entorno después de que el episodio termine\n",
    "\n",
    "    # Graficar la recompensa acumulada por episodio\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Desempeño del agente durante el entrenamiento\")\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluación del agente entrenado DESPUES DEL EPISODIO 1000\n",
    "    if episodes >= 1000:  # Solo hacer evaluación después de 1000 episodios\n",
    "        print(f\"Evaluación después del episodio 1000\")\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        done = False\n",
    "\n",
    "        # Evaluación con renderizado visual\n",
    "        while not done:\n",
    "            action = q_network.predict(np.expand_dims(state, axis=0)).flatten()\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            env.render()  # Mostrar la visualización gráfica del entorno\n",
    "\n",
    "        env.close()  # Cerrar el entorno al finalizar\n",
    "\n",
    "# Ejecutar el entrenamiento con 10000 episodios para una prueba rápida\n",
    "train(100)  # Cambié a 10000 episodios para mayor entrenamiento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
